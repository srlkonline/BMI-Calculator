# Web Scraping

-Collected data from a real website using requests and BeautifulSoup

-Parsed HTML content to extract relevant tables and elements

-Handled pagination and dynamic content (if any)

-Exported scraped data into a CSV file for further analysis


# Data Cleaning

-Removed duplicate entries to ensure data integrity

-Handled missing or null values using imputation or deletion

-Converted data types (e.g., dates, numeric values)

-Standardized inconsistent formats and categories

-Renamed columns for better readability and structure


# Exploratory Data Analysis (EDA)

-Analyzed distributions using histograms and box plots

-Explored relationships with scatter plots and correlation heatmaps

-Identified trends, outliers, and patterns in the dataset

-Grouped and aggregated data using groupby and pivot tables

-Summarized key findings and visualized with matplotlib and seaborn


# Tech Used

-Python 3.x

-Jupyter Notebook


# Libraries:

-requests, BeautifulSoup (Web Scraping)

-pandas, numpy (Data Wrangling)

-matplotlib, seaborn (Data Visualization)
